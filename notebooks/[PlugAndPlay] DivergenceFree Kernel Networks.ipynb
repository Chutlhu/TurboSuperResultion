{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "155841e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch running on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "torch.cuda.is_available()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "print('Torch running on:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b8adc9",
   "metadata": {},
   "source": [
    "## DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6354ecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data in numpy\n",
    "def load_turbo2D_simple_numpy(path_to_turbo2D, ds=4, img=42):\n",
    "    \"\"\"\n",
    "    ds : down-scaling factor (1 = nothing, 2=every 2 samples, ...)\n",
    "    img : index of the image (max is 55)\n",
    "    \n",
    "    returns: \n",
    "        X flatten coordinate grid (R^2)x2\n",
    "        y flatten target velocity field (R^2)x2\n",
    "    \"\"\"\n",
    "    \n",
    "    assert img < 55\n",
    "\n",
    "    IMGs = np.load(path_to_turbo2D)\n",
    "    img = img\n",
    "    X = IMGs[img,::ds,::ds,:2] / 255\n",
    "    U = IMGs[img,::ds,::ds,2:]\n",
    "\n",
    "    # normalize output\n",
    "    y = U.copy()\n",
    "    print('Y shape', y.shape)\n",
    "    print('Y min, max:', np.min(y), np.max(y))\n",
    "    y = y / np.max(np.abs(y))\n",
    "    print('after normalization, Y min, max:', np.min(y), np.max(y))\n",
    "\n",
    "    X = X.reshape(-1,2)\n",
    "    y = y.reshape(-1,2)\n",
    "\n",
    "    assert X.shape == y.shape\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "class Turbo2D_simple(Dataset):\n",
    "    \n",
    "    def __init__(self, path_to_turbo2D, device, ds=4, img=42):\n",
    "        \n",
    "        print('Dataset Turbo2D, img #', img)\n",
    "\n",
    "        IMGs = np.load(path_to_turbo2D)\n",
    "        X = IMGs[img,::ds,::ds,:2] / 255\n",
    "        U = IMGs[img,::ds,::ds,2:]\n",
    "\n",
    "        print(X.shape)\n",
    "        print(U.shape)\n",
    "\n",
    "        original_size = X.shape[0]\n",
    "        print('Original size', original_size)\n",
    "\n",
    "        # normalize output\n",
    "        y = U.copy()\n",
    "        print('Y shape', y.shape)\n",
    "        print('Y min, max:', np.min(y), np.max(y))\n",
    "        y = y / np.max(np.abs(y))\n",
    "        \n",
    "        print('after normalization, Y min, max:', np.min(y), np.max(y))\n",
    "\n",
    "        self.x = torch.from_numpy(X).float().to(device).view(-1,2)\n",
    "        self.y = torch.from_numpy(y).float().to(device).view(-1,2)\n",
    "\n",
    "        assert self.x.shape[0] == self.y.shape[0]\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx,:]\n",
    "        y = self.y[idx,:]\n",
    "        return (x, y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b04e16",
   "metadata": {},
   "source": [
    "*optional*: load groundtruth data in numpy.\n",
    "\n",
    "This might be usefull for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3cafb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y shape (64, 64, 2)\n",
      "Y min, max: -2.365612 2.941536\n",
      "after normalization, Y min, max: -0.8042097733972999 1.0\n",
      "Y shape (256, 256, 2)\n",
      "Y min, max: -2.382122 2.999472\n",
      "after normalization, Y min, max: -0.7941804424245333 1.0\n",
      "Img res: 64 Input batch: (4096, 2)\n",
      "Img res: 256 Input batch: (65536, 2)\n"
     ]
    }
   ],
   "source": [
    "path_to_turbo2D = '../data/2021-Turb2D_velocities.npy'\n",
    "img_idx = 42\n",
    "\n",
    "# optional: load groundtruth data in numpy.\n",
    "#           usefull for plotting\n",
    "Xlr, ulr = load_turbo2D_simple_numpy(path_to_turbo2D, ds=4, img=img_idx) # LR data\n",
    "Xhr, uhr = load_turbo2D_simple_numpy(path_to_turbo2D, ds=1, img=img_idx) # HR data\n",
    "\n",
    "L = int(Xlr.shape[0]**0.5)\n",
    "H = int(Xhr.shape[0]**0.5)\n",
    "\n",
    "print('Img res:', L, 'Input batch:', Xlr.shape) # LR data\n",
    "print('Img res:', H, 'Input batch:', Xhr.shape) # HR data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be895296",
   "metadata": {},
   "source": [
    "training data in pytorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "295c5cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Turbo2D, img # 42\n",
      "(64, 64, 2)\n",
      "(64, 64, 2)\n",
      "Original size 64\n",
      "Y shape (64, 64, 2)\n",
      "Y min, max: -2.365612 2.941536\n",
      "after normalization, Y min, max: -0.8042097733972999 1.0\n"
     ]
    }
   ],
   "source": [
    "trainset = Turbo2D_simple(path_to_turbo2D, device, ds=4, img=42)\n",
    "trainloader = DataLoader(trainset, batch_size=6666, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf15dad",
   "metadata": {},
   "source": [
    "# Basics building block of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6d3a7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fourier(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute Random Fourier Features:\n",
    "        sin(2*pi*B*x) and cos(2*pi*B*x)\n",
    "        B : matrix n_featuresx2\n",
    "        x : input vector n_batchx2        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, nfeat, scale):\n",
    "        super(Fourier, self).__init__()\n",
    "        self.b = nn.Parameter(torch.randn(2, nfeat)*scale, requires_grad=False)\n",
    "        self.pi = 3.14159265359 # better to hardcode this, rather than use np.pi\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.einsum('bc,cf->bf', 2*self.pi*x, self.b.to(x.device))\n",
    "        return torch.cat([torch.sin(x), torch.cos(x)], -1)\n",
    "\n",
    "    \n",
    "def LinearReLU(n_in, n_out):\n",
    "    \"\"\"\n",
    "    Simple block Linear + ReLU\n",
    "    \"\"\"\n",
    "    block = nn.Sequential(\n",
    "      nn.Linear(n_in, n_out),\n",
    "      nn.ReLU()\n",
    "    )\n",
    "    return block\n",
    "\n",
    "\n",
    "def LinearTanh(n_in, n_out):\n",
    "    \"\"\"\n",
    "    Simple block Linear + Tanh\n",
    "    # Tanh is C^\\infty and it may be better when computing high order derivates\n",
    "    \"\"\"\n",
    "    block = nn.Sequential(\n",
    "      nn.Linear(n_in, n_out),\n",
    "      nn.Tanh()\n",
    "    )\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85d16248",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple MLP\n",
    "        - dim_layes in a list [Nin] + [H_1] + [...] + [Nout]:\n",
    "            e.g. [2] + 3*[256] + [2]\n",
    "        - last_activation_fun : a nn.Module\n",
    "            e.g. nn.Tanh(), nn.Sigmoid(), or nn.ReLU()\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim_layers):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        num_layers = len(dim_layers)\n",
    "        \n",
    "        blocks = []\n",
    "        for l in range(num_layers-2):\n",
    "            blocks.append(LinearTanh(dim_layers[l], dim_layers[l+1]))\n",
    "            \n",
    "        blocks.append(nn.Linear(dim_layers[-2], dim_layers[-1]))\n",
    "        blocks.append(nn.Sigmoid())\n",
    "        self.network = nn.Sequential(*blocks)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afa6618a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DivFreeKernel(nn.Module):\n",
    "    \"\"\"\n",
    "    Given a Radial Basis Function $\\psi(x)$, the matrix-valued RBFs $\\Phi(x)$ in which\n",
    "    the vector fields defined by the columns are divergence-free is constructed as follows\n",
    "            $\\Phi_df(x) = (\\nabla \\nabla^T - \\nabla^2 I)\\psi(x)$ (1).\n",
    "    \n",
    "    In the 2D case and assuming that our MLP ~ RBFs (***STRONG ASSUMPTION HERE***), we can compute our vector field as follows.\n",
    "    \n",
    "    $\\Phi_df(x) = ( [ d/dxx   d/dxy  ] - [d/dxx + d/dyy           0        ] ) MLP(x)\n",
    "                  ( [ d/dyx   d/dyy  ] - [      0           d/dxx + d/dyy  ] )\n",
    "                \n",
    "                = ( [ -d/dyy    d/dxy  ]) Phi(x)\n",
    "                  ( [  d/dyx   -d/dxx  ])\n",
    "    \n",
    "    u = -dMLP(x)/dyy + dMLP(x)/dxy\n",
    "    v =  dMLP(x)/dxy - dMLP(x)/dxx\n",
    "    \n",
    "    \n",
    "    References:\n",
    "        - [Macedo et Casto,  2010](https://www.yumpu.com/en/document/read/37810994/learning-divergence-free-and-curl-free-vector-fields-with-matrix-)\n",
    "        - [Colin P. McNally, 2011](https://arxiv.org/pdf/1102.4852.pdf)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(DivFreeKernel, self).__init__()\n",
    "        pass\n",
    "    \n",
    "    def forward(self, y, x):\n",
    "        \n",
    "        # compute first order deriv\n",
    "        dy_xy = torch.autograd.grad(y, x, torch.ones_like(y), \n",
    "                                    create_graph=True,\n",
    "                                    retain_graph=True)[0]\n",
    "        # separate x and y components\n",
    "        dy_x, dy_y = dy_xy.split(1,-1)\n",
    "        \n",
    "        # compute secord order deriv wrt to x\n",
    "        dy_x_xy = torch.autograd.grad(dy_x, x, torch.ones_like(dy_x), \n",
    "                                      create_graph=True,\n",
    "                                      retain_graph=True)[0]\n",
    "        # compute secord order deriv wrt to y\n",
    "        dy_y_xy = torch.autograd.grad(dy_y, x, torch.ones_like(dy_y),\n",
    "                                      create_graph=True,\n",
    "                                      retain_graph=True)[0]\n",
    "        \n",
    "        dy_xx, dy_xy = dy_x_xy.split(1,-1)\n",
    "        dy_yx, dy_yy = dy_y_xy.split(1,-1)\n",
    "\n",
    "        # gather results in a matrix Bx2x2 in the form of Div-free kernel\n",
    "        # K1 = torch.cat([-dy_yy, dy_xy], dim=-1)[...,None]\n",
    "        # K2 = torch.cat([dy_yx, -dy_xx], dim=-1)[...,None]\n",
    "        # K = torch.cat([K1, K2], dim=-1)\n",
    "        # the columns of K make a divergence-free field\n",
    "        u =  dy_xy - dy_yy\n",
    "        v =  dy_xy - dy_xx\n",
    "        \n",
    "        u = torch.cat([u,v], dim=-1)\n",
    "        return u\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d2104d",
   "metadata": {},
   "source": [
    "Putting all the blocks together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b51d9cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DivFreeRFFNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, name, \n",
    "                    dim_mpl_layers,\n",
    "                    do_rff, f_nfeatures, f_scale, lam_pde=1, \n",
    "                    verbose=True):\n",
    "        \n",
    "        super(DivFreeRFFNet, self).__init__()\n",
    "        self.name = name\n",
    "        self.verbose = verbose\n",
    "\n",
    "        assert dim_mpl_layers[-1] == 1\n",
    "        \n",
    "        # regression/pinn network \n",
    "        self.do_rff = do_rff\n",
    "        if do_rff:\n",
    "            self.rff = Fourier(f_nfeatures, f_scale) # directly the random matrix 'cause of checkpoint and load\n",
    "            dim_mpl_layers[0] = dim_mpl_layers[0]*f_nfeatures\n",
    "            self.rff = Fourier(f_nfeatures, f_scale)\n",
    "\n",
    "        self.mlp = MLP(dim_mpl_layers)\n",
    "        self.div = DivFreeKernel()\n",
    "        \n",
    "        self.lam_pde = lam_pde # regularizer for divergence solft constraint\n",
    "\n",
    "        # for log of the losses\n",
    "        self.loss = []\n",
    "        self.loss_pde = []\n",
    "        self.loss_rec = []\n",
    "        \n",
    "        \n",
    "    def forward(self, xin): # x := BxC(Batch, InputChannels)\n",
    "        \n",
    "        xin.requires_grad_(True)\n",
    "        \n",
    "        ## Fourier features\n",
    "        if self.do_rff:\n",
    "            x = self.rff(xin) # Batch x Fourier Features\n",
    "            ## MLP\n",
    "            x = self.mlp(x)\n",
    "        else:\n",
    "            ## MLP\n",
    "            x = self.mlp(xin)\n",
    "            \n",
    "        # at this point x in the potential\n",
    "        potential = x\n",
    "        \n",
    "        # divergence free\n",
    "        div_free_uv = self.div(x, xin)\n",
    "        \n",
    "        return div_free_uv, potential\n",
    "    \n",
    "    def compute_ux(self, xin):\n",
    "        xin.requires_grad_(True)\n",
    "        y, Py = self.forward(xin)\n",
    "        du_xy = torch.autograd.grad(Py, xin, torch.ones_like(Py), create_graph=True)[0]       \n",
    "        return du_xy\n",
    "\n",
    "    def fit(self, trainloader, epochs=1000):\n",
    "        self.train()\n",
    "        optimiser = torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "        epoch = 0\n",
    "        while epoch < epochs or loss < 1e-6:\n",
    "            epoch += 1\n",
    "            current_loss = 0\n",
    "            batches = 0\n",
    "            for x_batch, y_batch in trainloader:\n",
    "                batches += 1\n",
    "                optimiser.zero_grad()\n",
    "                \n",
    "                # FORWARD\n",
    "                y_hat, potential_hat = self.forward(x_batch)\n",
    "                \n",
    "                # LOSS\n",
    "                loss_rec = (1/batches)*F.mse_loss(y_hat, y_batch)\n",
    "                \n",
    "                # check div=0\n",
    "                u, v = torch.split(y_hat,1,-1)\n",
    "                du_xy = torch.autograd.grad(u, x_batch, torch.ones_like(u), create_graph=True)[0]       \n",
    "                dv_xy = torch.autograd.grad(v, x_batch, torch.ones_like(v), create_graph=True)[0]\n",
    "                div_u_xy = du_xy[...,0] + dv_xy[...,1]\n",
    "                loss_pde = (1/batches)*torch.norm(div_u_xy)**2\n",
    "            \n",
    "                # the network is div-free by construction,\n",
    "                # so we optimize against the loss_rec only.\n",
    "                # We compute loss_pde for logging\n",
    "                loss = loss_rec # + self.lam_pde*loss_pde\n",
    "                current_loss +=  loss.item() - current_loss\n",
    "\n",
    "                self.loss_rec.append(loss_rec.item())\n",
    "                self.loss_pde.append(self.lam_pde*loss_pde.item())\n",
    "\n",
    "                # BACKWARD\n",
    "                loss.backward()\n",
    "                optimiser.step()\n",
    "                \n",
    "                # LOG\n",
    "                if self.verbose and (epoch % 100 == 0 or epoch == 1):\n",
    "                    print('Epoch: %4d, Loss: (rec: [%f] + %1.2f * div-free: [%f]) = %f' %\n",
    "                     (epoch, loss_rec.item(), self.lam_pde, loss_pde.item(), current_loss))\n",
    "\n",
    "        print('Done with Training')\n",
    "        print('Final error:', current_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2606a271",
   "metadata": {},
   "source": [
    "MLP + DivFree(autograd)  --- no RFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d10c5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(x):\n",
    "    return x.detach().cpu().numpy()\n",
    "\n",
    "def to_torch(x):\n",
    "    return torch.from_numpy(x).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd6135be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:    1, Loss: (rec: [19634.960938] + 1.00 * div-free: [0.079679]) = 19634.960938\n",
      "Epoch:  100, Loss: (rec: [58.602135] + 1.00 * div-free: [0.005027]) = 58.602135\n",
      "Epoch:  200, Loss: (rec: [20.528111] + 1.00 * div-free: [0.003738]) = 20.528111\n",
      "Epoch:  300, Loss: (rec: [10.999135] + 1.00 * div-free: [0.003436]) = 10.999135\n",
      "Epoch:  400, Loss: (rec: [7.075034] + 1.00 * div-free: [0.003312]) = 7.075034\n",
      "Epoch:  500, Loss: (rec: [4.955549] + 1.00 * div-free: [0.003055]) = 4.955549\n",
      "Epoch:  600, Loss: (rec: [3.621280] + 1.00 * div-free: [0.002849]) = 3.621280\n",
      "Epoch:  700, Loss: (rec: [2.708524] + 1.00 * div-free: [0.002792]) = 2.708524\n",
      "Epoch:  800, Loss: (rec: [2.054521] + 1.00 * div-free: [0.002714]) = 2.054521\n",
      "Epoch:  900, Loss: (rec: [1.572750] + 1.00 * div-free: [0.002636]) = 1.572750\n",
      "Epoch: 1000, Loss: (rec: [1.211449] + 1.00 * div-free: [0.002502]) = 1.211449\n",
      "Done with Training\n",
      "Final error: 1.211449384689331\n"
     ]
    }
   ],
   "source": [
    "# model (RFF + MLP)\n",
    "do_rff = True   # <--------\n",
    "fft_scale = 10  # <--------\n",
    "fft_nfeat = 256 # <--------\n",
    "\n",
    "nin = 2  # = x and y coordinates\n",
    "nout = 1 # = in the mlp we predict the potential, ux and uy components followed\n",
    "mlp_layers = [nin] + [256]*3 + [nout] # if do_rff, the first layer is 2*n_features, but this is already handled by the class construction\n",
    "\n",
    "model = DivFreeRFFNet('DivFreeNet_RFF', mlp_layers,\n",
    "                      do_rff, fft_nfeat, fft_scale, verbose=True)\n",
    "\n",
    "# TRAIN!\n",
    "model.to(device)\n",
    "model.fit(trainloader, epochs=1000) # we are not afraid to overfit the data, this is coord-based MLP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595e1d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "model.eval().to(device)\n",
    "ulr_pred, Plr_pred = model(to_torch(Xlr))\n",
    "uhr_pred, Phr_pred = model(to_torch(Xhr))\n",
    "\n",
    "ulr_pred = to_numpy(ulr_pred)\n",
    "Plr_pred = to_numpy(Plr_pred)\n",
    "\n",
    "uhr_pred = to_numpy(uhr_pred)\n",
    "Phr_pred = to_numpy(Phr_pred)\n",
    "\n",
    "# PLOT\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(131)\n",
    "plt.imshow(ulr_pred[:,0].reshape(L,L))\n",
    "plt.subplot(132)\n",
    "plt.imshow(ulr_pred[:,1].reshape(L,L))\n",
    "plt.subplot(133)\n",
    "plt.imshow(Plr_pred.reshape(L,L))\n",
    "plt.suptitle('Low Resolution')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(131)\n",
    "plt.imshow(uhr_pred[:,0].reshape(H,H))\n",
    "plt.subplot(132)\n",
    "plt.imshow(uhr_pred[:,1].reshape(H,H))\n",
    "plt.subplot(133)\n",
    "plt.imshow(Phr_pred.reshape(H,H))\n",
    "plt.suptitle('High Resolution')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
