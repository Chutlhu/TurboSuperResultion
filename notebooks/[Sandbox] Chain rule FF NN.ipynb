{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "033f3d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd3b4a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (network): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=10, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Linear(in_features=10, out_features=20, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Linear(in_features=20, out_features=30, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Linear(in_features=30, out_features=2, bias=True)\n",
      "      (1): Tanh()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def LinearTanh(n_in, n_out):\n",
    "    # do not work with ModuleList here either.\n",
    "    block = nn.Sequential(\n",
    "      nn.Linear(n_in, n_out),\n",
    "      nn.Tanh()\n",
    "    )\n",
    "    return block\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_layers):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        num_layers = len(dim_layers)\n",
    "        \n",
    "        blocks = []\n",
    "        for l in range(num_layers-1):\n",
    "            blocks.append(LinearTanh(dim_layers[l], dim_layers[l+1]))\n",
    "\n",
    "        self.network = nn.Sequential(*blocks)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "mlp_layers = [2] + [10] + [20] + [30] + [2]\n",
    "\n",
    "mlp = MLP(mlp_layers)\n",
    "print(mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a08e3b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3,2)\n",
    "y = mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "100495e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tanh = lambda x : torch.tanh(x)\n",
    "dtanh = lambda x : 1 - torch.tanh(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ac23ef72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.2374,  0.1247, -0.6819, -0.4599,  0.0659, -0.3426,  0.3905, -0.3083,\n",
       "         0.3608, -0.4472], requires_grad=True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.network[0][0].weight\n",
    "mlp.network[0][0].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4f303536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "def get_wb(model,depth):\n",
    "    return model[depth][0].weight, model[depth][0].bias\n",
    "\n",
    "\n",
    "W = []\n",
    "B = []\n",
    "\n",
    "for i in range(4):\n",
    "    w, b = get_wb(mlp.network,i)\n",
    "    W.append(w)\n",
    "    B.append(b)\n",
    "    \n",
    "print(len(W))\n",
    "print(len(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fbfc7f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 10])\n",
      "torch.Size([3, 20])\n",
      "torch.Size([3, 30])\n",
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "a = W[-1]@W[-2]@W[-3]@W[-4]\n",
    "tmp = x@W[-4].T + B[-4]\n",
    "in1 = dtanh(tmp)\n",
    "in2 = dtanh(tanh(in0)@W[-3].T + B[-3])\n",
    "in3 = dtanh(tanh(in2)@W[-2].T + B[-2])\n",
    "in4 = dtanh(tanh(in3)@W[-1].T + B[-1])\n",
    "print(in1.shape)\n",
    "print(in2.shape)\n",
    "print(in3.shape)\n",
    "print(in4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3993313c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "3828bbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4901e-08, -1.4901e-08],\n",
      "        [-2.9802e-08, -2.9802e-08],\n",
      "        [ 1.4901e-08,  1.4901e-08],\n",
      "        [-7.4506e-09, -1.4901e-08],\n",
      "        [-1.4901e-08, -1.4901e-08],\n",
      "        [ 0.0000e+00,  0.0000e+00],\n",
      "        [ 3.7253e-09,  7.4506e-09],\n",
      "        [ 0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00]], grad_fn=<SubBackward0>)\n",
      "tensor([[ 0.0000e+00, -6.0322e-02],\n",
      "        [ 7.4506e-09, -8.9252e-02],\n",
      "        [-7.4506e-09, -5.4100e-02],\n",
      "        [ 7.4506e-09, -4.8782e-02],\n",
      "        [ 0.0000e+00, -5.1549e-02],\n",
      "        [ 0.0000e+00,  8.1362e-02],\n",
      "        [-3.7253e-09, -1.3981e-02],\n",
      "        [ 0.0000e+00, -7.2688e-02],\n",
      "        [ 0.0000e+00, -7.2188e-02],\n",
      "        [-7.4506e-09, -5.1812e-02]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def LinearTanh(n_in, n_out):\n",
    "    # do not work with ModuleList here either.\n",
    "    block = nn.Sequential(\n",
    "      nn.Linear(n_in, n_out),\n",
    "      nn.Tanh()\n",
    "    )\n",
    "    return block\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_layers):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        num_layers = len(dim_layers)\n",
    "        \n",
    "        blocks = []\n",
    "        for l in range(num_layers-1):\n",
    "            blocks.append(LinearTanh(dim_layers[l], dim_layers[l+1]))\n",
    "\n",
    "        self.network = nn.Sequential(*blocks)\n",
    "        \n",
    "        self.sigma = lambda x : torch.tanh(x)\n",
    "        self.dsigma = lambda x : 1-torch.tanh(x)**2\n",
    "        self.ddsigma = lambda x : -2*torch.tanh(x)*(1-torch.tanh(x)**2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "    \n",
    "    def get_wb(self, depth):\n",
    "        return self.network[depth][0].weight, self.network[depth][0].bias\n",
    "    \n",
    "    def compute_ux(self, x):\n",
    "        W1, b1 = self.get_wb(0)\n",
    "        d1 = self.dsigma(x @ W1.T + b1)\n",
    "        a1 = self.sigma(x @ W1.T + b1)\n",
    "        \n",
    "        z = d1 @ W1\n",
    "        \n",
    "#         W2, b2 = self.get_wb(1)\n",
    "#         d2 = self.dsigma(a1 @ W2.T + b2)\n",
    "#         a2 = self.sigma(a1 @ W2.T + b2)\n",
    "#         z = (d2 @ W2) * d1) @ W1\n",
    "        \n",
    "#         W3, b3 = self.get_wb(2)\n",
    "#         d3 = self.dsigma(a2 @ W3.T + b3)\n",
    "#         a3 = self.sigma(a2 @ W3.T + b3)\n",
    "#         z = (d3 @ W3 * d2) @ W2 * d1) @ W1\n",
    "        \n",
    "#         W4, b4 = self.get_wb(3)\n",
    "#         d4 = self.dsigma(a3 @ W4.T + b4)\n",
    "#         a4 = self.sigma(a3 @ W4.T + b4)\n",
    "        \n",
    "#         z = ((((d4 @ W4 * d3) @ W3 * d2) @ W2 * d1) @ W1\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    def compute_uxx(self, x):\n",
    "        W1, b1 = self.get_wb(0)\n",
    "        d1 = self.ddsigma(x@W1.T + b1)\n",
    "        z = d1 @ W1**2\n",
    "        return z\n",
    "    \n",
    "dim_in = 2\n",
    "mlp_layers = [dim_in] + [1]\n",
    "\n",
    "mlp = MLP(mlp_layers)\n",
    "\n",
    "x = torch.randn(10,dim_in)\n",
    "x.requires_grad_(True)\n",
    "y = mlp(x)\n",
    "\n",
    "ux1_auto = torch.autograd.grad(y, x, torch.ones_like(y), retain_graph=True, create_graph=True)[0]\n",
    "ux1_analy = mlp.compute_ux(x)\n",
    "print(ux1_auto - ux1_analy)\n",
    "\n",
    "ux2_auto = torch.autograd.grad(ux1_auto[:,0], x, torch.ones_like(ux1_auto[:,0]))[0]\n",
    "ux2_analy = mlp.compute_uxx(x)\n",
    "print(ux2_analy - ux2_auto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aadfff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
